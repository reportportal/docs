## Auto-Analysis of launches

The analysis feature of the ReportPortal makes it possible for the application to check and pass part of the routine duties by itself.

Auto-analysis take a part of your routine work and defines the reason of the test item failure and sets:

* a defect type;
* a link to BTS _(in case if it exists)_;
* comment _(in case if it exists)_;

The process of Auto-Analysis is based on previous user-investigated users' results  using the Machine Learning.  

An auto-analyzer is presented by combination of two services: ElasticSearch and Analyzer service. 

There are several ways to use an analyzer in our application:

* Use  the ReportPortal Analyzer: **manual** (analysis is switched on only for chosen launch manually) or **auto** (analysis is switched on after the launch finishing automatically);

* Implement and configure your own custom Analyzer and do not deploy ReportPortal service Analyzer;

* Do not use any Analyzers at all and do an analytical routine by yourself;

### ReportPortal Analyzer. How the Auto-Analysis is working

ReportPortal's auto-analyzer allows users to reduce time spent on test execution investigation by analyzing test failures in automatic mode. For that reason you can deploy the  ReportPortal with a service Analyzer by adding an info about this service in a docker-compose file. Default analysis component is running along with ElasticSearch which is used for test logs indexing.
For effective using Auto–Analysis you should come through several stages. 

#### Create an analytical base in the ElasticSearch 

First of all you need to create an analytical base. For that you should start to analyze test results manually. 

All test items with defect type *(except defect type “To investigate”)* which have been analyzed manually or automatically by ReportPortal are sent to the Elastic Search. 

The following info is sent:

* An item ID;
* A log higher than a log level Error (log >= 40 000);
* Issue type;
* Flag: “Analyzed by” (where shown by whom the test item has been analyzed by a user or by ReportPortal);
* A launch name;
* Unique ID;

Service Analyzer composes a search query to ElasticSearch that consists of the following data fields:
* LogMessage;
* Launch ID;
* UniqueID test item; 
* Defect type;

Each log entry along with its defect type is saved to ElasticSearch in form of a separate document. All documents created compose an Index. The more test results index has, the more accurate results will be generated by the end of the analysis process.

>**Note:** You need to send at least **N logs (where N is a value of parameter MinDocFreq)** to ElasticSearch for start to analyze test results automatically.
>
>If you do not sure how many documents(logs) are contained in the Index on that moment, you can check it. 
>For that, perform following actions: 
>* Uncommented Service ElasticSearch ports in docker-compose file or add them: 9200:9200; 
>* Restart Service ElasticSearch with new docker-compose; 
>* Send a request to ElasticSeaarch: 
>    * how many documents in the Index: GET http://localhost:9200/_cat/indices?v  
>    * Detailed information:  POST http://localhost:9200/{project_name}/_search

Test items of a launch in Debug mode are not sent to service Analyzer. If test item is deleted or moved to the Debug mode, the it is removed from the Index.

#### Auto-Analysis process

After your Index have been completed. You can start to use the auto-analysis feature.

Analysis can be launched automatically (via Project Settings) or manually (via menu on All launches view). After process is started, all items with defect type “To investigate” with logs (>= 40 000) from the analyzed launch are picked and sent to the  Service Analyzer and the service ElasticSearch for investigations.

Here there is a simplified procedure of the Auto-analysis via ElasticSearch.

Data of test items with defect type “To investigate” and logs (>= 40 000) are sent to service Analyzer: 
* UniqueID;          
* IsAutoAnalyzed: true/false    bool;   
* IssueType;        
* Original Issue Type; 
* LogID;    
* LogLevel 
* Log Message  

Service Analyzer forms a query with a log data to the ElasticSearch.The query are contained following info:
* LogMessage;
* Launch ID;
* UniqueID test item; 

Then ElasticSearch receives log message and divides it on the terms (words) with a tokenizer and calculates the importance of each term (word). For that ElasticSearch computes TF-IDF for each term (word) in analyzed log. If level of a term importance is low, the ElasticSearch ignores it. 

>**Note:**
>
>*Term frequency (TF)* – how many time term (word) is used in analyzed log;
>
>*Document frequency (DF)* – in how many documents this term (word) is used in Index;
>
>*TF-IDF (TF — term frequency, IDF — inverse document frequency)* — a statistical measure used to assess the importance of a term (word)  in the context of a log that is part of a Index. The weight of a term (word)   is proportional to the amount of use of this term (word)   in the analyzed log, and inversely proportional to the frequency of term (word)   usage in Index.

The term (word) with the highest level of importance is the term (word) that is used  very frequently  in analyzed log and moderately in the Index.
 
After all important terms are defined, Elastic search calculates the level of equality between an analyzed log and each log in the Index.  For each log from the Index is calculated a score. 

>**Note:**
>
>How calculated a score:
>
>**score(q,d)** = 
>
>         coord(q,d) -
>         SUM ( 
>                tf(t in d),  
>               idf(t)²,  
>                t.getBoost(), 
>                       ) (t in q)
>Where:
>*	score(q,d) is the relevance score of log “d” for query “q”.
>*	coord(q,d) is the coordination factor: the percent of words equality between analyzed log and particular log from the ElasticSearch.
>*	The sum of the weights for each word “t” in the query “q” for log “d”.
>	* tf(t in d) is a frequency of the word in the analyzed log.
>	* idf(t) is the inverse frequency of the word in all saved logs in the Index.
>	* t.getBoost() is the boost that has been applied to the query. The higher priority for logs with:
>	   * The same Launch name;
>	   * The same UID;
>	   * Manual analysis;
       
The ElasticSearch returns to the service Analyzer 10 logs with the highest score. Service Analyzer sums scores of logs with the same Defect type and calculated group defect type ( defect type with the highest score). This defect type is assigned to the analyzed test item. 

>**Note:**
In case test item has several logs, service Analyzer computes group defect type for each log, and test item gets defect type with the higest score. 

A defect comment and a link to BTS of the item with highest score from this group come to the analyzed item.

So this is how Auto-Analysis works and defines the most relevant defect type on base of the previous investigations. We give an ability to our users to configure auto-analysis manually.

### Auto-analysis Settings

All settings and configurations of Analyzer and ElasticSearch are situated on a separate tab on Project settings.

1. Login into ReportPortal instance as Administrator or project member with PROJECT MANAGER or LEAD role on the project;

2. Come on Project Settings, choose Auto-Analysis section;

[ ![Image]( Images/userGuide/analyzeLaunches/AASettings.png) ]( https://youtu.be/GaaRpHAw7iQ)

In this section user can perform following actions:

1.	Switch ON/OFF auto-analysis;

2.	Choose a base for analysis (All launches/ Launches with the same name);

3.	Configure ElasticSearch settings;

4.	Remove/Generate ElasticSearch index.


#### Switch ON/OFF automatic analysis;

To activate the "Auto-Analysis" functionality in a project, perform the following steps:

1. Login ReportPortal instance as Administrator or project member with PROJECT MANAGER or LEAD role on the project.

2. Select ON  in "Auto-Analysis" selector on Project settings / Auto-analysis section.

3. Click the "Submit" button. Now "Auto-Analysis" will start as soon as any launch finishes.

#### Base for analysis (All launches/ Launches with the same name);

You can choose which results from previous runs should be considered in Auto –Analysis for defining the failure reason.

There two options:

* All launches;

* Launches with the same name;

If you choose **“All launches”**, test results in launch will have analyzed on base of all data in Elastic search of the project. 

If you choose **“Launches with the same name”**, test results in launch will have analyzed on base of all data in Elastic search that have the same Launch name.

You can choose those configurations via Project configuration or from the list of actions on All launches view.

#### Configure ElasticSearch settings

Also we give possibility for our users to configure 4 main parameters of ElasticSearch manually:

*	**MinDocFreq** - the minimum frequency of the saved logs in ElasticSearch (index) in which word from analyzed log should be used. If the log count is below the specified value, that word will be ignored for AA in the analyzed log. The more often the word appears in index, the lower it weights. Min value 1, max value 10. 
*	**MinTermFreq** - the minimum frequency of the word in the analyzed log. If the word count is below the specified value, this word will be ignored for AA. The more often the word appears in the analyzed log, the higher it weights. Min value 1, max value 10.
*	**MinShouldMatch** - percent of words equality between analyzed log and particular log from the ElasticSearch. If a log from ElasticSearch has the value less then set, this log will be ignored for AA. Min value 50, max value 100.
*	**Number of log lines** - the number of first lines of log message that should be considered in ElasticSearch. Only choosen number of log line will be saved in ElasticSearch. Possible values: 2, 3, 4, 5, All.  In case you choose “ALL”, full text of log will be saved in ElasticSearch Index.

Parameters **MinDocFreq** and **MinTermFreque** are involved in the calculation of TF-IDF coefficient. So you can normalized log and reduce importance of frequent but useless words in a log (f.e. "Java" or prepositions...) manually. 
 
> **Note:**
For  option **“Only current launch” (base for Auto-analysis)**, parameter MinDocFreq is always 1.

Parameter **MinShouldMatch** is involved in the calculation of a score. It is a minimum value for coord(q,d) (the percent of words equality between an analyzed log and a particular log from the ElasticSearch). So you can increase search hardness and choose a minimum level of similarity that is required.

With the parameter **Number of log lines** - you can  write the root cause of a test fail in the first lines and configure analyzer to take into account only the required lines. 

With these 4 parameters you can configure an accuracy of analysis that you need. For you facilities we have prepared 3 pre-sets with values:

*	*Light* - search conditions are freer. You will get more results, but with the less level of similarity;
*	*Moderate* - "happy medium";
*	*Strict* - search conditions are strict. You will get less results, but with the higher level of similarity;

#### Remove/Generate ElasticSearch index

There two possible action that can be performed under Index in ElasticSearch. 

You can **remove the Index from ElasticSearch** and all logs with there defect type will be delete. ML will be set to zero. All data with your investigations will be deleted from the ElasticSearch. For creating a new one you could start to investigate test results manually or generate data based on previous results on the project once again. 

>**Note:**
Your investigations in ReportPortal will not be changed. The operation concerns only ElasticSearch base.

[ ![Image]( Images/userGuide/analyzeLaunches/AAremove.png) ]( https://youtu.be/GsRK5agYPxs)

Another option, you can **generate the Index in ElasticSearch**. In case of generation, all data will be removed from ElasticSearch and new one will be generated based on all previous investigations on the project in accordance with current analysis settings. 

In the end of the process you will receive a letter with info about the end of the process and with number of items that will be appeared in ElasticSearch.

You can use index generation for several goals. For example, assume two hypothetical situations when index genertaion can be used:

* by accident you remove index, but now you want to restore it. 

>**Note:** 
New base will be generated in accordance with logs and settings that are existing on the moment of performing operation. So index before removing and index after generation can be different. 

* you have changed a parameter **Number of log lines** for 3.  But your existing index contains logs with value ALL. You can generate new index, old index will be removed, and new one will be generated. Logs in new index will contain 3 lines;  

[ ![Image]( Images/userGuide/analyzeLaunches/AAgenerate.png) ]( https://youtu.be/Iq6tf40J_Wk)

We strongly do not recommended use auto-analysis until new index will be generated.

#### Manual analysis

Analysis can be launched manually. To start the analysis manually, perform the following steps:

1. Navigate to the "Launches" page.

2. Select the "Analysis" option from the context menu next to the selected
    launch name.

3. Choose a scope of a previous results on base of which test items should be auto-analyzed.  Default is the one that is chosen on setting page, but you can change it manually.

Via this menu you have obility to choose 3 options unlike on Project Settings: 

* All launches;

* Launches with the same name;

* Only current launch;

Options **All launches** and **Launches with the same name** are working the same as on project settings. 
If you choose **Only current launch**, the system is analyzing the test items of chosen launch only on base of already investigated date of the this launch.

4. Choose which items from launch should be analyzed:

* Only To investigated; 
* Items analyzed automatically (by AA);
* Items analyzed manually;

In case the user chooses **Only To investigate items** -  the system is analyzing only items with defect type "To investigate" in chosen launch;

In case the user chooses **Items analyzed automatically (by AA)** - the system is analyzing only items that have been already analyzed by auto-analysis. The results of previous run of analysis will be set to zero and items will be analyzed once again.

In case the user chooses **Items analyzed manually** - the system is analyzing only items that have been already analyzed by user manually. The results of previous run of analysis will be set to zero and items will be analyzed once again.

In case of multi combination - the system is analyzing results in dependence with chosen options.

>**Note:**
The Ignore flag is saved. If item has flag **Ignore in AA**, it will not be re-analyzed.

> **Note:**
For option **Only current lunch** you can not choose *Items analyzed automatically (by AA)* and *Items analyzed manually* simultaneously.

4. Click the "Analysis" button. Now "Auto-Analysis" will start.

Any launches with an active analyzing process will be marked with the "Analysis" label. 

 [ ![Image]( Images/userGuide/analyzeLaunches/AAmanual.png) ](https://youtu.be/ulJ16fRT2Jw)


### Label AA

When test item is analyzed by the ReportPortal, a label "AA" is set on the test item on a Step Level. You can filter results with a parameter “Analysed by RP (AA)”

[ ![Image](Images/userGuide/analyzeLaunches/Auto-Analysis-AA.png) ](Images/userGuide/analyzeLaunches/Auto-Analysis-AA.png)

### Ignore in Auto-Analysis

If you don't want to save some test items in ElasticSearch, you can "Ignore it in Auto-Analysis". For that you can choose this action in “Defect type editor” pop-up:

[ ![Image](Images/userGuide/analyzeLaunches//Auto-Analysis-IgnorePopUp.png) ](Images/userGuide/analyzeLaunches/Auto-Analysis-IgnorePopUp.png)

Or from the action list for several test items:

[ ![Image](Images/userGuide/analyzeLaunches/Auto-Analysis-IgnoreActionList.png) ](Images/userGuide/analyzeLaunches/Auto-Analysis-IgnoreActionList.png)

When you choose “Ignore in AA”, logs of the chosen item are removed from the ElasticSearch. 


### Custom Analyzer

If you do not want to use our ReportPortal auto-analyzer, you can implement and configure custom analyzer. The example of analyzer is under the [link]( https://github.com/pbortnik/example-custom-analyzer) 

## Copy results from a previous run

In case you do not want to use Auto-Analyzer we provide you a possibility to get a results from previous runs.  So that you can set for a test item: Defect type, linked bug and comment from previous run at once. For that you can hit a button "Copy defect from #" in the last test item and copy a defect from a the last but one test item with defect type.

[ ![Image](Images/userGuide/analyzeLaunches/CopyResults.png)](Images/userGuide/analyzeLaunches/CopyResults.png)

or hit a button "Send defect to #" from the not the last items and send defect to the last test item ( if it can has a defect type).

[ ![Image](Images/userGuide/analyzeLaunches/SendReultsResults.png)](Images/userGuide/analyzeLaunches/SendReultsResults.png)

