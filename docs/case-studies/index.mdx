---
sidebar_position: 16
sidebar_label: Case Studies
---

# Case Studies

## Reducing regression time by 50%

EPAM helps a Canadian retail company to reverse-engineer their legacy IBM-based store management system to a modern tech stack. As part of this project, ReportPortal was deployed as a centralized test reporting system.

**Challenges**

- Unavailble environments (15VMs) blocked by aggregation scripts
- High risk of aggregation fail: 1 in 10 aggregations fails. In case of a fail, the whole regression should be re-run.
- Constant regression fails move weekly releases for 1 day
- Lack of information for investigation: no screnshots/no history/no structure/no all info
- Duplicated analysis efforts: missing history of test cases and known issues

**Highlights**

- Simplified test run reporting by integrating the test framework with ReportPortal.io
- Distributed test execution data for root cause analysis: logs/screenshots/ attachments
- Provided a possibility to triage failed items (AI-based and manual)
- Provided clear reporting for non-technical stakeholders
- Real-time reporting
- Save on early reaction: team result analysis right after execution started in real time
- Collaborative results analysis
- Test Case History helped to identify flaky test cases
- Extended ML Analyzer

<media-view src={require('./img/CaseStudy1.png')} alt="Case Study Reducing regression time by 50%" />

## Increasing test automation stability and visibility

**Challenges**

- 2,000 unique tests had low stability (25% passing rate)
- Complex and inconvenient test results reporting
- QA team didn’t have capacity to analyze the results of failed tests
- Automation results were ignored by decision makers: no analysis of the causes of failed tests, no trust in the automation testing process
- Lack of visibility into failed tests and failure causes
- Absence of clear reporting of QA engineers’ workload and performance

**Highlights**

Integration with ReportPortal.io allowed the client to:

- Collect history for previous test runs
- Identify passing, failing, and unstable tests
- Select stable tests in a separate run
- Assign unstable test for refactoring and add them to a separate run for implementation
- Configure ReportPortal.io charts to track a refactoring progress
- Accelerate failure analysis through access to related logs, screenshots, and attachments in one place

**Results**

- Automation stability improved from 25% to 95%
- Analysis efforts of QA engineers decreased by 10 times
- Client stakeholders use ReportPortal.io data to make release decisions
- ReportPortal.io became the main tool for tracking test automation progress and health

<media-view src={require('./img/CaseStudy2.png')} alt="Case Study Increasing test automation stability and visibility" />


## Reducing regression analysis efforts

**Challenges**

- Test analysis could only start after full execution was completed (4 hours wasted daily)
- All test failures had to be analyzed manually
- No visibility into causes for tests failures
- Absence of history and trends of test failures
- No tools to manage team workload
- Test execution reports were done manually (1 hour of daily efforts)

**Highlights**

- Real-time analysis during test runs: results available after the first job execution, saving team capacity and providing an early reaction
- Automatic re-run of failed tests provided additional value and saved up to 5.5 of team’s hours per day
- About 20% of defects previously analyzed manually are being updated automatically through ML capabilities
- Clear visibility into the number of new /existing production defects, auto test related issues, and environment related issues
- Full understanding of application quality, correct planning of maintenance time, and transparent communication of environment instability based on real-time statistics
- History of tests execution helps to analyze causes of test failures more efficiently
- Improved task management due to possibility to plan work allocation and track tests assigned to each team member
- Real-time dashboards were tailored to client’s KPIs, giving full transparency of test execution results

<media-view src={require('./img/CaseStudy3.png')} alt="Case Study Reducing regression analysis efforts" />


## Improving test automation stability

**Challenges**

- Complex, manual test runs
- Low stability of regression for unclear reasons (60% passing rate)
- Unclear reporting for non-technical stakeholders, leading to the lack of transparency in test automation results and progress
- Test automation feedback is unclear, unreliable, incomprehensible, and insufficient to decide to push the app to production

**Highlights**

By integrating the test framework with ReportPortal.io, EPAM's team provided:

- Simplified test runs
- Key info for the manual root cause analysis of test failures, such as logs, screenshots, attachments
- A possibility to triage failed items (AI-based and manual)
- Clear reporting for non-technical stakeholders

**Results**

- Improved automation stability from 60% to 77% in one sprint
- Discovered that most failures were caused by environment issues and reduced the number of such failures from 20% to 2%
- Reduced test automation results analysis effort by 45%
- Provided clear and comprehensive test automation reporting: number of test cases, regression passing rate, reasons for failures, product status

<media-view src={require('./img/CaseStudy4.png')} alt="Case Study Improving test automation stability" />
